{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uY8hoCo_4jJbmR0TPMYJCwWgGnxjmQtp",
      "authorship_tag": "ABX9TyNA5ihJXOgd9Tiab9WPaTIQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hydaspex/chatbot_sagemaker/blob/main/ChatBot_SageMaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LnDSmWzVMEW"
      },
      "outputs": [],
      "source": [
        "!pip install \"sagemaker==2.175.0\" --upgrade --quiet\n",
        "!pip install awscli"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sagemaker config: Insert your own data\n",
        "!aws configure"
      ],
      "metadata": {
        "id": "l-kNhlqdfetm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insert your own sagemaker arn (formatted as below)\n",
        "Arn= \"arn:aws:iam::XXXXXXXXXXXX:role/XX\"\n"
      ],
      "metadata": {
        "id": "pNhhdR6VXnM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "sess = sagemaker.Session()\n",
        "# sagemaker session bucket -> used for uploading data, models and logs\n",
        "# sagemaker will automatically create this bucket if it not exists\n",
        "sagemaker_session_bucket=None\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    # set to default bucket if a bucket name is not given\n",
        "    sagemaker_session_bucket = sess.default_bucket()\n",
        "\n",
        "try:\n",
        "    role = Arn\n",
        "except ValueError:\n",
        "    iam = boto3.client('iam')\n",
        "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
        "\n",
        "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
        "\n",
        "print(f\"sagemaker role arn: {role}\")\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")"
      ],
      "metadata": {
        "id": "wFv3okiRXrNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
        "\n",
        "# retrieve the llm image uri\n",
        "llm_image = get_huggingface_llm_image_uri(\n",
        "  \"huggingface\",\n",
        "  version=\"0.9.3\"\n",
        ")\n",
        "\n",
        "# print ecr image uri\n",
        "print(f\"llm image uri: {llm_image}\")\n"
      ],
      "metadata": {
        "id": "pSs8v4VeoQJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "# sagemaker config\n",
        "instance_type = \"ml.g5.12xlarge\"\n",
        "number_of_gpu = 4\n",
        "health_check_timeout = 1600\n",
        "\n",
        "# Define Model and Endpoint configuration parameter\n",
        "config = {\n",
        "  'HF_MODEL_ID': \"OpenAssistant/pythia-12b-sft-v8-7k-steps\", # model_id from hf.co/models\n",
        "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
        "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
        "  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
        "  'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
        "}\n",
        "\n",
        "# create HuggingFaceModel with the image uri\n",
        "llm_model = HuggingFaceModel(\n",
        "  role=role,\n",
        "  image_uri=llm_image,\n",
        "  env=config\n",
        ")\n"
      ],
      "metadata": {
        "id": "v8Hty6cdodkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy model to an endpoint\n",
        "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
        "llm = llm_model.deploy(\n",
        "  initial_instance_count=1,\n",
        "  instance_type=instance_type,\n",
        "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
        "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
        ")\n"
      ],
      "metadata": {
        "id": "jPsI-FTbo1sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = llm.predict({\n",
        "\t\"inputs\": \"\"\"<|prompter|>What are some cool ideas to do in the summer?<|endoftext|><|assistant|>\"\"\"\n",
        "})\n",
        "\n",
        "print(chat[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CqJhumAotpd",
        "outputId": "c032842c-deac-44ca-ea2d-0388b4bc20f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|prompter|>What are some cool ideas to do in the summer?<|endoftext|><|assistant|>There are many fun and exciting things you can do in the summer! Here are some ideas:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define payload\n",
        "prompt=\"\"\"<|prompter|>How can i stay more active during winter? Give me 3 tips.<|endoftext|><|assistant|>\"\"\"\n",
        "\n",
        "# hyperparameters for llm\n",
        "payload = {\n",
        "  \"inputs\": prompt,\n",
        "  \"parameters\": {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.7,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"<|endoftext|>\"]\n",
        "  }\n",
        "}\n",
        "\n",
        "# send request to endpoint\n",
        "response = llm.predict(payload)\n",
        "\n",
        "# print(response[0][\"generated_text\"][:-len(\"<human>:\")])\n",
        "print(response[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "ZkDEoGhMe0Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gradio to create a web interface\n",
        "\n",
        "!pip install gradio  --upgrade"
      ],
      "metadata": {
        "id": "kI4K-OUKfMNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# hyperparameters for llm\n",
        "parameters = {\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.7,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 50,\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"repetition_penalty\": 1.03,\n",
        "    \"stop\": [\"<|endoftext|>\"]\n",
        "  }\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Chat with Amazon SageMaker\")\n",
        "    with gr.Column():\n",
        "        chatbot = gr.Chatbot()\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                message = gr.Textbox(label=\"Chat Message Box\", placeholder=\"Chat Message Box\", show_label=False)\n",
        "            with gr.Column():\n",
        "                with gr.Row():\n",
        "                    submit = gr.Button(\"Submit\")\n",
        "                    clear = gr.Button(\"Clear\")\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        # convert chat history to prompt\n",
        "        converted_chat_history = \"\"\n",
        "        if len(chat_history) > 0:\n",
        "          for c in chat_history:\n",
        "            converted_chat_history += f\"<|prompter|>{c[0]}<|endoftext|><|assistant|>{c[1]}<|endoftext|>\"\n",
        "        prompt = f\"{converted_chat_history}<|prompter|>{message}<|endoftext|><|assistant|>\"\n",
        "\n",
        "        # send request to endpoint\n",
        "        llm_response = llm.predict({\"inputs\": prompt, \"parameters\": parameters})\n",
        "\n",
        "        # remove prompt from response\n",
        "        parsed_response = llm_response[0][\"generated_text\"][len(prompt):]\n",
        "        chat_history.append((message, parsed_response))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    submit.click(respond, [message, chatbot], [message, chatbot], queue=False)\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch(share=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8qY0EMI5f09M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To clean up, we can delete the model and endpoint.\n",
        "\n",
        "llm.delete_model()\n",
        "llm.delete_endpoint()"
      ],
      "metadata": {
        "id": "Gfu4wMu_zTXD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}